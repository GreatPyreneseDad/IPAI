#!/usr/bin/env python3
"""
Test Ollama Integration

Simple test script to verify Ollama + Llama 3.2 integration
with the IPAI system.
"""

import asyncio
import sys
import logging
import json
import aiohttp
from pathlib import Path
from typing import Dict, Optional, AsyncGenerator, List, Any, Union
from dataclasses import dataclass
from datetime import datetime

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

# Import the Ollama client directly since we're testing the integration
class OllamaClient:
    """Ollama API client for Llama 3.2 integration"""
    
    def __init__(self, host: str = "http://localhost:11434", model: str = "llama3.2:latest", timeout: float = 30.0):
        self.host = host.rstrip('/')
        self.model = model
        self.timeout = timeout
        self.session = None
        
    async def _get_session(self):
        """Get or create aiohttp session"""
        if self.session is None or self.session.closed:
            connector = aiohttp.TCPConnector(limit=10, limit_per_host=5)
            timeout = aiohttp.ClientTimeout(total=self.timeout)
            self.session = aiohttp.ClientSession(connector=connector, timeout=timeout)
        return self.session
    
    async def generate(self, prompt: str, max_tokens: int = 1024, temperature: float = 0.7, top_p: float = 0.9) -> str:
        """Generate response from Ollama"""
        session = await self._get_session()
        
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "num_predict": max_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "stop": ["User:", "Human:", "\n\nUser:", "\n\nHuman:"]
            }
        }
        
        try:
            async with session.post(f"{self.host}/api/generate", json=payload) as response:
                if response.status == 200:
                    result = await response.json()
                    return result.get("response", "").strip()
                else:
                    error_text = await response.text()
                    raise Exception(f"Ollama API error {response.status}: {error_text}")
        except aiohttp.ClientError as e:
            raise Exception(f"Ollama connection error: {e}")
    
    async def stream_generate(self, prompt: str, max_tokens: int = 1024, temperature: float = 0.7, top_p: float = 0.9) -> AsyncGenerator[str, None]:
        """Stream response from Ollama"""
        session = await self._get_session()
        
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": True,
            "options": {
                "num_predict": max_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "stop": ["User:", "Human:", "\n\nUser:", "\n\nHuman:"]
            }
        }
        
        try:
            async with session.post(f"{self.host}/api/generate", json=payload) as response:
                if response.status == 200:
                    async for line in response.content:
                        if line:
                            try:
                                data = json.loads(line.decode('utf-8'))
                                if 'response' in data:
                                    token = data['response']
                                    if token:
                                        yield token
                                if data.get('done', False):
                                    break
                            except json.JSONDecodeError:
                                continue
                else:
                    error_text = await response.text()
                    raise Exception(f"Ollama API error {response.status}: {error_text}")
        except aiohttp.ClientError as e:
            raise Exception(f"Ollama connection error: {e}")
    
    async def check_model(self) -> bool:
        """Check if model is available"""
        session = await self._get_session()
        
        try:
            async with session.get(f"{self.host}/api/tags") as response:
                if response.status == 200:
                    data = await response.json()
                    models = [model['name'] for model in data.get('models', [])]
                    return self.model in models
                return False
        except aiohttp.ClientError:
            return False
    
    async def pull_model(self) -> bool:
        """Pull model if not available"""
        session = await self._get_session()
        
        payload = {"name": self.model}
        
        try:
            async with session.post(f"{self.host}/api/pull", json=payload) as response:
                if response.status == 200:
                    async for line in response.content:
                        if line:
                            try:
                                data = json.loads(line.decode('utf-8'))
                                if data.get('status') == 'success':
                                    return True
                            except json.JSONDecodeError:
                                continue
                return False
        except aiohttp.ClientError:
            return False
    
    async def close(self):
        """Close the session"""
        if self.session and not self.session.closed:
            await self.session.close()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def test_ollama_connection():
    """Test basic Ollama connection"""
    print("üîç Testing Ollama connection...")
    
    try:
        client = OllamaClient(
            host="http://localhost:11434",
            model="llama3.2:latest"
        )
        
        # Check if model is available
        model_available = await client.check_model()
        print(f"üìã Model llama3.2:latest available: {model_available}")
        
        if not model_available:
            print("‚¨áÔ∏è  Attempting to pull model...")
            success = await client.pull_model()
            if success:
                print("‚úÖ Model pulled successfully")
            else:
                print("‚ùå Failed to pull model")
                return False
        
        # Test basic generation
        print("ü§ñ Testing basic generation...")
        response = await client.generate("Hello, how are you?", max_tokens=50)
        print(f"üìù Response: {response}")
        
        # Test streaming
        print("üì° Testing streaming...")
        full_response = ""
        async for token in client.stream_generate("Tell me a short joke", max_tokens=100):
            full_response += token
            print(token, end="", flush=True)
        print(f"\nüìù Full streamed response: {full_response}")
        
        await client.close()
        return True
        
    except Exception as e:
        print(f"‚ùå Ollama connection test failed: {e}")
        return False


async def test_simple_gct_integration():
    """Test basic GCT-style prompting with Ollama"""
    print("\nüß† Testing GCT-style prompting...")
    
    try:
        client = OllamaClient(
            host="http://localhost:11434",
            model="llama3.2:latest"
        )
        
        # Test GCT-inspired prompt
        gct_prompt = """You are a coherence-focused AI assistant that helps users build internal consistency, accumulated wisdom, moral activation, and social belonging.

When responding:
1. Consider the user's internal consistency (alignment between thoughts and actions)
2. Draw on wisdom and experience for guidance
3. Encourage values-aligned action
4. Foster connection and belonging

User: I'm feeling confused about my life direction and need some guidance.
    
    # Test 1: Basic Ollama connection
    ollama_ok = await test_ollama_connection()
    if not ollama_ok:
        print("‚ùå Basic Ollama tests failed. Please ensure:")
        print("  1. Ollama is running: ollama serve")
        print("  2. Llama 3.2 is available: ollama pull llama3.2")
        return False
    
    # Test 2: GCT LLM Interface
    gct_ok = await test_gct_llm_interface()
    if not gct_ok:
        print("‚ùå GCT LLM interface tests failed")
        return False
    
    # Test 3: Crisis intervention
    crisis_ok = await test_crisis_intervention()
    if not crisis_ok:
        print("‚ùå Crisis intervention tests failed")
        return False
    
    print("\nüéâ All tests passed! Ollama + Llama 3.2 integration is working correctly.")
    print("\nüìã Integration Summary:")
    print("  ‚úÖ Ollama connection established")
    print("  ‚úÖ Llama 3.2 model available")
    print("  ‚úÖ Basic text generation working")
    print("  ‚úÖ Streaming generation working")
    print("  ‚úÖ GCT coherence integration working")
    print("  ‚úÖ Triadic processing working")
    print("  ‚úÖ Crisis intervention working")
    print("  ‚úÖ Performance monitoring working")
    
    print("\nüöÄ Your IPAI system is ready to use with Ollama + Llama 3.2!")
    return True


if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Tests interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nüí• Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)